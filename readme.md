# ğŸ§ ğŸ“š Modular RAG Pipeline â€” Powered by FastAPI + LangChain

Welcome to the **RAG (Retrieval-Augmented Generation)** project!

This is a modular and scalable system built to handle document ingestion, vector search, memory-powered interactions, and custom prompts â€” all wrapped in a FastAPI interface with a user-friendly frontend.

> âš™ï¸ Built for speed, flexibility, and production-grade applications.

![App Screenshot](pic.png)

---

## ğŸš€ Features

- ğŸ“„ Upload and process PDF documents  
- âœ‚ï¸ Chunk documents and embed them using HuggingFace models  
- ğŸ§  Vector store powered by FAISS  
- ğŸ” Conversational memory with LangChain's `ConversationBufferMemory`  
- âš™ï¸ Dynamic RAG pipeline powered by **Groq + LLaMA 3** (blazing fast inference)  
- ğŸŒ REST API and simple HTML frontend via **FastAPI**  
- ğŸ“¬ Query your documents and get relevant, context-aware answers  
- ğŸ’¬ Customizable system prompts (coming soon)

---

## âœ… Current Progress

The core system includes:

- âœ… LangChain with Groqâ€™s LLaMA 3 LLM  
- âœ… HuggingFace sentence-transformers (`all-MiniLM-L6-v2`) for document embeddings  
- âœ… FAISS for in-memory vector storage  
- âœ… Conversational memory (chat history preserved across queries)  
- âœ… Document upload and question-answering from PDFs  
- âœ… FastAPI backend with a basic but functional HTML UI  

---

## ğŸ§ª Example Usage

```python
from pathlib import Path
from rag_pipeline import RagPipeline

# Initialize the pipeline
pipeline = RagPipeline()

# Process a PDF and prepare the QA chain
qa_chain = pipeline.run_pipeline(Path("example.pdf"))

# Ask a question
response = qa_chain.run("What is this document about?")
print(response)
