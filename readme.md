# ğŸ§ ğŸ“š Modular RAG Pipeline â€” Powered by FastAPI + LangChain

Welcome to my **RAG (Retrieval-Augmented Generation)** project!  
This is a modular and scalable system designed to accept document inputs, allow custom system prompts, and retrieve information with memory â€” all via a clean FastAPI interface.

> âš™ï¸ Built for speed, flexibility, and continuous improvement.

---

## ğŸš€ Features

- ğŸ—‚ï¸ Upload documents (PDFs)
- ğŸ§© Chunk documents and embed them using HuggingFace
- ğŸ§  Store vectorized documents with FAISS
- ğŸ§¾ Customizable system prompt (coming soon)
- ğŸ¤– Powered by Groq + LLaMA 3 for lightning-fast LLM inference
- ğŸ” Retrieval QA over your private documents
- ğŸ–¥ï¸ FastAPI-ready for production deployment

---

## âœ… Current Progress

Iâ€™ve completed a working **RAG pipeline** with:

- LangChain + Groq LLM (LLaMA 3)
- HuggingFace sentence-transformers for embeddings
- FAISS for vector storage
- Simple document upload and question answering

```python
from pathlib import Path
from rag_pipeline import RagPipeline

pipeline = RagPipeline()
qa_chain = pipeline.run_pipeline(Path("example.pdf"))
response = qa_chain.run("What is this document about?")
print(response)
